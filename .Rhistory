library("haven")
n_sim <- 650000
data_input <- import_och_tvatt("//e75a0679/sakta/UTV/SU/DATA/Tariffanalysdata/df_vb_alla_skador_20170101.sas7bdat",divtest="N")[[3]]
# Variable selction gross
data_input <- data_input %>% dplyr::select(c(freq,BOYTA, Bolag , Fast_alder , Byggnadstyp , Brevobjekt , Alder, dur))
data_idx <- sample(c(1:dim(data_input)[1] ), size = n_sim, replace = FALSE)
df$all <- as.data.frame(data_input[data_idx,])
# factors
df$all$Bolag <- as.factor(df$all$Bolag)
df$all$Byggnadstyp <- as.factor(df$all$Byggnadstyp)
df$all$Brevobjekt <- as.factor(df$all$Brevobjekt)
}
if (data == "auspriv"){
load(paste("Data/ausprivauto0405.rda", sep = ""))
n_sim <- length(ausprivauto0405)
data_idx <- sample(c(1:dim(ausprivauto0405)[1] ), size = n_sim, replace = FALSE)
df$all <- as.data.frame(ausprivauto0405[data_idx,]) %>% dplyr::select(-c("ClaimOcc","ClaimAmount"))
# Naming to fit algo
colnames( df$all$ClaimNb ) <- "freq"
colnames( df$all$Exposure)  <- "dur"
# Factors
df$all$VehAge <- as.factor( df$all$VehAge)
df$all$VehBody  <- as.factor( df$all$VehBody)
df$all$Gender    <- as.factor( df$all$Gender)
df$all$DrivAge    <- as.factor( df$all$DrivAge)
}
colnames( df$all$ClaimNb )
names( df$all$ClaimNb )
df$all$ClaimNb
df$all
n_sim
ausprivauto0405
n_sim <- nrow(ausprivauto0405)
n_sim
data_idx <- sample(c(1:dim(ausprivauto0405)[1] ), size = n_sim, replace = FALSE)
df$all <- as.data.frame(ausprivauto0405[data_idx,]) %>% dplyr::select(-c("ClaimOcc","ClaimAmount"))
# Naming to fit algo
names( df$all$ClaimNb ) <- "freq"
names( df$all$Exposure)  <- "dur"
df$all
names( df$all$ClaimNb )
colnames( df$all$ClaimNb )
# Naming to fit algo
colnames( df$all$ClaimNb ) <- "freq"
# Naming to fit algo
colnames( df$all) <- "freq"
data_idx <- sample(c(1:dim(ausprivauto0405)[1] ), size = n_sim, replace = FALSE)
df$all <- as.data.frame(ausprivauto0405[data_idx,]) %>% dplyr::select(-c("ClaimOcc","ClaimAmount"))
colnames( df$all)
colnames( df$all)["ClaimNb"]
colnames( df$all)[["ClaimNb"]]
colnames( df$all)[which(colnames( df$all) == "ClaimNb")]
# Naming to fit algo
colnames( df$all)[which(colnames( df$all) == "ClaimNb")] <- "freq"
colnames( df$all)[which(colnames( df$all) == "Exposure")]  <- "dur"
colnames( df$all)
for (data in c("auspriv", "beMTPL", "norauto")){ # "freMTPL", REAL
rm(list = setdiff(ls(),c("data","date")))
suffix <- data
new_models <- TRUE
new_univar_effects <- TRUE
new_twoway_effects <- TRUE
save <- TRUE
scoring_boosting_factors <- TRUE
plot_folder <- paste("Plottar/",suffix,"_",date, sep="")
dir.create(plot_folder)
# Modeling hyperparameters ---------------------------------------------------------
n_trees_mu <- 1000
tree_depth_mu <- 2
learning_rate_mu <- 0.025
train_frac <- 0.8
cal_frac <- 0.3 # Of the training data, how much for calibration of marginal effects?
source("modeling.r")
}
View(ausprivauto0405)
model.freq_gbm.ref
df$all
effect_model
facts
facts <- c("VehValue",
"VehAge",
"VehBody",
"Gender",
"DrivAge")
num_facts
univariate_pdp_data
if (new_twoway_effects == TRUE){
effect_model <- models$raw_gbm
n_trees <- models$raw_gbm.ntrees
for (i in 1:length(facts)){
for (j in 1:max(1,(i-1)) ){
if (i == 1 & j ==1 ){H_stat <- data.frame()}
# Friedmans H-stat f?r varje faktorkombination
H_stat <- bind_rows(H_stat, data.frame(Faktor_1 = facts[j],
Faktor_2 = facts[i],
H_stat = interact.gbm(effect_model,
data= boosting_df$train,
n.trees = n_trees,
i.var = c(facts[j],facts[i]) )))
}
print(paste("Done with interactions for", facts[i]))
}
n_interactions <- 5
max_grid <- 10
top_interactions <- H_stat %>% filter(H_stat <1) %>% arrange(desc(H_stat)) %>% top_n(n_interactions) %>% arrange(Faktor_1)
interaction_effects <- list()
interaction_effects$top_interactions <- top_interactions
for (i in 1:length(top_interactions$Faktor_1)) {
if (top_interactions$Faktor_1[i] %in% num_facts & top_interactions$Faktor_2[i] %in% num_facts){max_grid <- 10}
else{max_grid <-  min(nrow(unique(boosting_df$train[top_interactions$Faktor_1[i]] )),
nrow(unique(boosting_df$train[top_interactions$Faktor_2[i]] )))}
interaction_effects[[top_interactions$Faktor_1[i]]][[top_interactions$Faktor_2[i]]] <- partial(
effect_model,
pred.fun=pgbm_raw,
pred.var = c(top_interactions$Faktor_1[i], top_interactions$Faktor_2[i]),
n.trees= n_trees,
recursive = FALSE,
chull = TRUE,
grid.resolution = max_grid)
}
}
# Load or save data
if (save == TRUE){
save(interaction_effects, file = paste("Data/Interaction_effects_",suffix,"_raw.RData", sep = ""))
}else{
load(paste("Data/Interaction_effects_",suffix,"_raw.RData", sep = ""))
}
if (scoring_boosting_factors == TRUE){
factor_update <- list()
old_factor_effect <- list()
factor_levels <- list()
new_factor <- list()
univariate_pdp_data_complete <- list()
interaction_effects_complete <- list()
train_factors_new <- data.frame(matrix(rep(1,nrow(boosting_df$train)*(length(num_facts)+2) ), nrow = nrow(boosting_df$train)))
cal_factors_new <- data.frame(matrix(rep(1,nrow(boosting_df$cal)*(length(num_facts)+2) ), nrow = nrow(boosting_df$cal)))
test_factors_new <-data.frame(matrix(rep(1,nrow(boosting_df$test)*(length(num_facts)+2) ), nrow = nrow(boosting_df$test)))
names(train_factors_new) <- c(num_facts, "init_pred","freq")
names(cal_factors_new) <- c(num_facts, "init_pred","freq")
names(test_factors_new) <- c(num_facts, "init_pred","freq")
## Univariate
for (fact in num_facts){
model_name <- "Ref_gbm"
# Factor range
range <- min(df$all[fact]):max(df$all[fact])
univariate_pdp_data[[fact]]$factor_val <- round(univariate_pdp_data[[fact]]$factor_val)
factor_update[[fact]] <- as.numeric( approx(univariate_pdp_data[[fact]]$factor_val,
univariate_pdp_data[[fact]][[model_name]],
xout=range)[["y"]])
factor_update[[fact]][is.na(factor_update[[fact]])] <- mean(boosting_df$train$freq)
# Saving interpolated PDP-values
univariate_pdp_data_complete[[fact]] <- data.frame(factor_val = range,
Ref_gbm_interpol =  factor_update[[fact]] )
train_factors_new[fact] <- factor_update[[fact]][boosting_df$train[[fact]]  - min(range) + 1 ]
cal_factors_new[fact] <- factor_update[[fact]][boosting_df$cal[[fact]]  - min(range) + 1 ]
test_factors_new[fact] <- factor_update[[fact]][boosting_df$test[[fact]]  - min(range) + 1 ]
}
## Interactions
for (k in 1:length(top_interactions$Faktor_1)){
fact_1 <- top_interactions$Faktor_1[k]
fact_2 <- top_interactions$Faktor_2[k]
temp_interaction_effects <- interaction_effects[[fact_1]][[fact_2]]
temp_interaction_effects <- data.frame(temp_interaction_effects,idx=(1:nrow(temp_interaction_effects)) )
# Factor range
if (is.numeric(df$all[[fact_1 ]])){
range1 <- sort( unique(df$all[fact_1])[[fact_1]] )
}else{
range1 <- unique(df$all[fact_1])[[fact_1]]
}
if (is.numeric(df$all[[fact_2]])){
range2 <- sort( unique(df$all[fact_2])[[fact_2]] )
}else{
range2 <- unique(df$all[[fact_2]])
}
range_grid <- data.frame(expand.grid(range1,range2), pred = NA )
# If any of the two factors are categorical, matching exact, otherwise nearest neighbor
for (i in 1: nrow(range_grid)){
if (is.numeric(df$all[[top_interactions$Faktor_1[k]]])){
temp_fact1_val <- temp_interaction_effects[[fact_1]][which.min(abs(temp_interaction_effects[[fact_1]] - range_grid[i,"Var1"]))]
temp_interaction_effects_sub  <- temp_interaction_effects[temp_interaction_effects[[fact_1]] == temp_fact1_val,]
}else{
temp_interaction_effects_sub  <- temp_interaction_effects[temp_interaction_effects[[fact_1]] == range_grid[i,"Var1"],]
}
if (is.numeric(df$all[[top_interactions$Faktor_2[k]]])){
range_grid[i,"pred"] <- temp_interaction_effects_sub$yhat[which.min(abs(temp_interaction_effects_sub[[fact_2]] - range_grid[i,"Var2"]))]
}else{
range_grid[i,"pred"] <- temp_interaction_effects_sub$yhat[which(temp_interaction_effects_sub[[fact_2]] == range_grid[i,"Var2"])]
}
}
inter_fact_name <-  paste(fact_1,fact_2,sep="")
for (i in 1:nrow(boosting_df$train)){
train_factors_new[i,inter_fact_name] <-  (range_grid[intersect(which(range_grid$Var1 == boosting_df$train[[fact_1]][i]), which(range_grid$Var2 == boosting_df$train[[fact_2]][i])) ,])$pred
if (any(i == round( nrow(boosting_df$train) - (1:9)* nrow(boosting_df$train)/10 ) )){
print(paste("Done with ",percent(i/nrow(boosting_df$train))," of training" ))
}
}
for (i in 1:nrow(boosting_df$cal)){
cal_factors_new[i,inter_fact_name] <-  (range_grid[intersect(which(range_grid$Var1 == boosting_df$cal[[fact_1]][i]), which(range_grid$Var2 == boosting_df$cal[[fact_2]][i])) ,])$pred
if (any(i == round( nrow(boosting_df$cal) - (1:9)* nrow(boosting_df$cal)/10 ) )){
print(paste("Done with ",percent(i/nrow(boosting_df$cal))," of calibration" ))
}
}
for (i in 1:nrow(boosting_df$test)){
test_factors_new[i,inter_fact_name] <-  (range_grid[intersect(which(range_grid$Var1 == boosting_df$test[[fact_1]][i]), which(range_grid$Var2 == boosting_df$test[[fact_2]][i])) ,])$pred
if (any(i == round( nrow(boosting_df$test) - (1:9)* nrow(boosting_df$test)/10 ) )){
print(paste("Done with ",percent(i/nrow(boosting_df$test))," of test" ))
}
}
print(paste("Done with interaction between ", fact_1, " and ", fact_2))
}
# Adding init, dur and y-values
train_factors_new["freq"] <- boosting_df$train$freq
cal_factors_new["freq"] <- boosting_df$cal$freq
test_factors_new["freq"] <- boosting_df$test$freq
# Adding init, dur and y-values
train_factors_new["dur"] <- boosting_df$train$dur
cal_factors_new["dur"] <- boosting_df$cal$dur
test_factors_new["dur"] <- boosting_df$test$dur
# Init pred is hereof legacy reasons...
train_factors_new["init_pred"] <- mean(boosting_df$train$freq)
cal_factors_new["init_pred"] <-  mean(boosting_df$train$freq)
test_factors_new["init_pred"] <- mean(boosting_df$train$freq)
boosting_df$train_factors <- train_factors_new
boosting_df$cal_factors <- cal_factors_new
boosting_df$test_factors <- test_factors_new
# Save or load
if (save == TRUE){save(boosting_df, file=paste("Data/Boost_data_",suffix,"_raw.RData", sep = ""))
}else{
load(paste("Data/Boost_data_",suffix,"_raw.RData", sep = ""))}
}
invisible(partial(
effect_model,
pred.fun=pgbm_raw,
pred.var = c(top_interactions$Faktor_1[i], top_interactions$Faktor_2[i]),
n.trees= n_trees,
recursive = FALSE,
chull = TRUE,
grid.resolution = max_grid))
partial(
effect_model,
pred.fun=pgbm_raw,
pred.var = c(top_interactions$Faktor_1[i], top_interactions$Faktor_2[i]),
n.trees= n_trees,
recursive = FALSE,
chull = TRUE,
grid.resolution = max_grid)
invisible(
)
invisible(partial(models$raw_gbm,
pred.fun=pgbm_raw,
chull = TRUE,
pred.var = c(fact),
n.trees= models$raw_gbm.ntrees,
quantiles = TRUE,
recursive = FALSE,
probs = c(1:30)/30 ))
suppressMessages( partial(models$raw_gbm,
pred.fun=pgbm_raw,
chull = TRUE,
pred.var = c(fact),
n.trees= models$raw_gbm.ntrees,
quantiles = TRUE,
recursive = FALSE,
probs = c(1:30)/30 ))
p
train_factors_new
tree_control = rpart.control(minbucket=10, cp=0.00001)
all_facts <- names(boosting_df$train_factors %>% dplyr::select(-c("freq","dur")))
boosting_df$train_factors_final <- boosting_df$train_factors
boosting_df$cal_factors_final <- boosting_df$cal_factors
boosting_df$test_factors_final <-  boosting_df$test_factors
for (fact in all_facts){
model_trees <- formula(eval(paste("freq ~ ", fact, sep="")))
tree_deep <- rpart(model_trees,
data = boosting_df$train_factors %>% mutate(freq = freq/dur),
method = "anova",
weights = dur,
control = tree_control )
# Pruning
prune_cp_glm <- tree_deep$cptable[which.min(tree_deep$cptable[, "xerror"]), "CP"]
tree_temp <- prune.rpart(tree_deep, cp = prune_cp_glm)
## Updating final factors
boosting_df$train_factors_final[fact] <- predict(tree_temp, newdata = boosting_df$train_factors)
boosting_df$cal_factors_final[fact] <- predict(tree_temp, newdata = boosting_df$cal_factors)
boosting_df$test_factors_final[fact] <- predict(tree_temp, newdata = boosting_df$test_factors)
# Updating PDP-values
if (fact %in% facts) {
temp_data <-  data.frame(val = univariate_pdp_data_complete[[fact]]$Ref_gbm_interpol)
names(temp_data) <- fact
univariate_pdp_data_complete[[fact]]$Final_model <- predict(tree_temp, newdata = temp_data)*mean(boosting_df$train_factors$freq)
}
}
all_facts
# Adding back categoricals
boosting_df$train_factors_final <- data.frame(boosting_df$train_factors_final, boosting_df$train[, cat_facts])
boosting_df$cal_factors_final <- data.frame(boosting_df$cal_factors_final, boosting_df$cal[, cat_facts])
boosting_df$test_factors_final <- data.frame(boosting_df$test_factors_final, boosting_df$test[, cat_facts])
final_factors <- apply(boosting_df$train_factors_final,2, FUN= function(x) length(unique(x)))
boosting_df$train_factors_final
final_factors <- apply(boosting_df$train_factors_final,2, FUN= function(x) length(unique(x)))
final_factors <- names(final_factors[final_factors>1])
final_factors <- final_factors[!final_factors %in% c("dur", "freq")]
# Models
model.freq_glm.final <- formula(eval(paste("freq ~ factor(", paste(final_factors, collapse = ") + factor(" ), ") + offset(log(dur))" , sep="")))
model.freq_glm.final_lasso <- formula(eval(paste("freq ~ factor(", paste(final_factors, collapse = ") + factor(" ), ")" , sep="")))
models$final$vanilla  <- glm(model.freq_glm.final,
data = boosting_df$train_factors_final,
family = quasipoisson(link = "log"))
summary(models$final$vanilla )
glmnet_data <- model.matrix(model.freq_glm.final_lasso , boosting_df$train_factors_final )
glmnet_data_y <- as.matrix(boosting_df$train_factors_final %>% dplyr::select(c(freq,dur)))
models$final$lasso  <- cv.glmnet(x = glmnet_data,
y = glmnet_data_y[,"freq"],
intercept=T ,
offset = log(glmnet_data_y[,"dur"]),
family = poisson(link = "log"),
alpha = 1,
nfolds = 5,
lambda = seq(0, 0.1, length.out=100))
# Finalizing factors
plot(models$final$lasso)
best_lambda <- unique(models$final$lasso$lambda[min(models$final$lasso$cvm) == models$final$lasso$cvm])
# Final factors -----------------------------------------------------------
coef(models$final$lasso)
glmnet_data_y
# Predictions  -------------------------------------------------
pred$train$boosted_glm$vanilla <- sapply(as.numeric(predict.glm(models$final$vanilla, newdat=boosting_df$train_factors_final, type="response", newoffset=boosting_df$train_factors_final$dur)) , function(x) min(x,2))
pred$cal$boosted_glm$vanilla <- sapply(as.numeric(predict.glm(models$final$vanilla, newdat=boosting_df$cal_factors_final, type="response"), newoffset=boosting_df$cal_factors_final$dur), function(x) min(x,2))
pred$test$boosted_glm$vanilla <- sapply(as.numeric(predict.glm(models$final$vanilla, newdat=boosting_df$test_factors_final, type="response") , newoffset=boosting_df$test_factors_final$dur ) , function(x) min(x,2))
pred$train$boosted_glm$lasso <- sapply(as.numeric(predict(models$final$lasso,  newx = model.matrix(model.freq_glm.final_lasso , boosting_df$train_factors_final ), type = "response",  s = best_lambda, newoffset = log(boosting_df$train_factors_final$dur) )), function(x) min(x,2))
pred$cal$boosted_glm$lasso <- sapply(as.numeric(predict(models$final$lasso, newx = model.matrix(model.freq_glm.final_lasso , boosting_df$cal_factors_final ), type = "response", s = best_lambda, newoffset = log(boosting_df$cal_factors$dur))), function(x) min(x,2))
pred$test$boosted_glm$lasso <- sapply(as.numeric(predict(models$final$lasso, newx = model.matrix(model.freq_glm.final_lasso , boosting_df$test_factors_final ), type = "response", s = best_lambda, newoffset = log(boosting_df$test_factors$dur))) , function(x) min(x,2))
# Balance check  -------------------------------------------------
mean(boosting_df$train$freq)
mean(pred$train$boosted_glm$vanilla)
mean(pred$train$boosted_glm$lasso)
# Extracting lasso values
row_names <- rownames(coef(models$final$lasso))
factors <- sub(".*\\((.*)\\).*", "\\1", row_names)
factor_values <- sub(".*\\)(.*)$", "\\1", row_names)
coef_values <- coef(models$final$lasso)
coef_values <- as.numeric(as.matrix(coef_values))
coef_values[is.na(coef_values)] <- 0
# Adjusting model values
for (fact in num_facts){
temp_pdp_val <- round(univariate_pdp_data_complete[[fact]]$Final_model ,6)
temp_coef_level <- round(as.numeric(factor_values[which(factors==fact) ]),6)
temp_coef_val <- round(as.numeric(coef_values[which(factors==fact) ]),6)
temp_new_pdp_val <- rep(mean(boosting_df$train$freq), length(temp_pdp_val))
for (i in 1:length(temp_pdp_val)){
if( any(temp_pdp_val[i] == temp_coef_level)){
idx <- which(temp_pdp_val[i] == temp_coef_level)
temp_new_pdp_val[i] <- temp_coef_val[idx]
}
}
univariate_pdp_data_complete[[fact]]$Final_model_lasso <- temp_new_pdp_val
}
univariate_pdp_data_complete
if (save == TRUE){
save(models, file = paste("Data/Models_",suffix,".RData", sep = ""))
save(boosting_df, file = paste("Data/Boost_data_",suffix,".RData", sep = ""))
save(pred, file = paste("Data/Predictions_",suffix,".RData", sep = ""))
save(univariate_pdp_data_complete, file = paste("Data/PDP_uni_",suffix,".RData", sep = ""))
}
for (fact in num_facts){
p <-  univariate_pdp_data_complete[[fact]] %>%
ggplot(aes(x=factor_val))+
geom_line(aes(y=Final_model, color="red"))+
geom_line(aes(y=Ref_gbm_interpol , color="grey"), lty=2) +
geom_line(aes(y=Final_model_lasso, color="blue"))+
geom_abline(intercept = mean(df$train$freq),slope=0, color="grey", alpha=0.5)+
#xlim(xlim[1],xlim[2])+
labs(x= fact,
y="PDP" )+
scale_colour_manual(name = '',
values =c('black'='black','red'='red','grey'='grey','blue'='blue'),
labels = c('Linear','Final GLM','GBM (PDP)','After trees')
)+
scale_y_continuous(sec.axis = sec_axis( trans= ~./mean(df$train$freq), name="Boosting factor")) +
theme_classic() +
theme(legend.position ="bottom")
ggsave(filename = paste(plot_folder,"/PDP_boost_",fact , ".png",sep=""), plot = p, dpi = 300,width = 10, height = 8)
}
# Predictive performance --------------------------------------------------
model_names <- c("Intercept","GBM", "Linear", "Final GLM (no lasso)", "Final GLM")
MSEP_cal <- data.frame( Model= model_names,
MSEP=round(c( mean((mean(boosting_df$train$freq) - boosting_df$cal$freq) ^2),
mean((pred$cal$ref - boosting_df$cal$freq) ^2),
mean((pred$cal$init - boosting_df$cal$freq) ^2),
mean((pred$cal$boosted_glm$vanilla - boosting_df$cal$freq)^2),
mean((pred$cal$boosted_glm$lasso - boosting_df$cal$freq)^2)),4),
Deviance = round(c(deviance(rep(mean(boosting_df$train$freq),length(boosting_df$cal$freq)) , pred_phi=NULL, boosting_df$cal$freq, res = FALSE),
deviance(pred$cal$ref, pred_phi=NULL, boosting_df$cal$freq, res = FALSE),
deviance(pred$cal$init, pred_phi=NULL, boosting_df$cal$freq, res = FALSE),
deviance(pred$cal$boosted_glm$vanilla , pred_phi=NULL, boosting_df$cal$freq, res = FALSE),
deviance(pred$cal$boosted_glm$lasso , pred_phi=NULL, boosting_df$cal$freq, res = FALSE)),4),
Fidelity= round(c(0,
cor(pred$cal$ref, pred$cal$ref),
cor(pred$cal$init, pred$cal$ref),
cor(pred$cal$boosted_glm$vanilla , pred$cal$ref),
cor(pred$cal$boosted_glm$lasso ,pred$cal$ref)),2)
)
MSEP_test <- data.frame( Model=model_names,
MSEP=round(c( mean((mean(boosting_df$train$freq) - boosting_df$test$freq) ^2),
mean((pred$test$ref - boosting_df$test$freq) ^2),
mean((pred$test$init - boosting_df$test$freq) ^2),
mean((pred$test$boosted_glm$vanilla - boosting_df$test$freq) ^2),
mean((pred$test$boosted_glm$lasso - boosting_df$test$freq) ^2)),4),
Deviance = round(c(deviance(rep(mean(boosting_df$train$freq),length(boosting_df$cal$freq)) , pred_phi=NULL, boosting_df$test$freq, res = FALSE),
deviance(pred$test$ref, pred_phi=NULL, boosting_df$test$freq, res = FALSE),
deviance(pred$test$init, pred_phi=NULL, boosting_df$test$freq, res = FALSE),
deviance(pred$test$boosted_glm$vanilla , pred_phi=NULL, boosting_df$test$freq, res = FALSE),
deviance(pred$test$boosted_glm$lasso , pred_phi=NULL, boosting_df$test$freq, res = FALSE)),4) ,
Fidelity= round(c(0,
cor(pred$test$ref, pred$test$ref),
cor(pred$test$init, pred$test$ref),
cor(pred$test$boosted_glm$vanilla , pred$test$ref),
cor(pred$test$boosted_glm$lasso ,pred$test$ref)),2)
)
MSEP_test$MSEP <- round(MSEP_test$MSEP/MSEP_test$MSEP[2],4)
MSEP_cal$MSEP <- round(MSEP_cal$MSEP/MSEP_cal$MSEP[2],4)
MSEP_test$Deviance <- round(MSEP_test$Deviance/MSEP_test$Deviance[2],4)
MSEP_cal$Deviance <- round(MSEP_cal$Deviance/MSEP_cal$Deviance[2],4)
MSEP_cal
learning_rate_mu
tree_depth_mu
for (data in c("auspriv", "beMTPL", "norauto")){ # "freMTPL", REAL
rm(list = setdiff(ls(),c("data","date")))
suffix <- data
new_models <- TRUE
new_univar_effects <- TRUE
new_twoway_effects <- TRUE
save <- TRUE
scoring_boosting_factors <- TRUE
plot_folder <- paste("Plottar/",suffix,"_",date, sep="")
dir.create(plot_folder)
# Modeling hyperparameters ---------------------------------------------------------
n_trees_mu <- 1000
tree_depth_mu <- 2
learning_rate_mu <- 0.01
train_frac <- 0.8
cal_frac <- 0.3 # Of the training data, how much for calibration of marginal effects?
source("modeling.r")
}
### IMPORT freMTPL ###
if (data == "freMTPL"){
n_sim <- 650000
data_input <- read.csv("//e75a0679/sakta/UTV/SU/Program/Analys/Boosting GLM/Boosting-GLM/Data/freMTPL2freq.csv")
data_idx <- sample(c(1:dim(data_input)[1] ), size = n_sim, replace = FALSE)
df$all <- as.data.frame(data_input[data_idx,])
# Corrections according to Schelldorfer and W?thrich (2019)
df$all$ClaimNb <-  pmin( df$all$ClaimNb,4)
df$all$Exposure <- pmin( df$all$Exposure,1)
# Naming to fit algo
colnames( df$all)[2] <- "freq"
colnames( df$all)[3] <- "dur"
# Factors
df$all$Area <- as.factor( df$all$Area)
df$all$VehBrand  <- as.factor( df$all$VehBrand)
df$all$VehGas    <- as.factor( df$all$VehGas)
df$all$Region    <- as.factor( df$all$Region)
}
n_sim
for (data in c( "beMTPL", "norauto")){ # "auspriv","freMTPL", REAL
rm(list = setdiff(ls(),c("data","date")))
suffix <- data
new_models <- TRUE
new_univar_effects <- TRUE
new_twoway_effects <- TRUE
save <- TRUE
scoring_boosting_factors <- TRUE
plot_folder <- paste("Plottar/",suffix,"_",date, sep="")
dir.create(plot_folder)
# Modeling hyperparameters ---------------------------------------------------------
n_trees_mu <- 1000
tree_depth_mu <- 2
learning_rate_mu <- 0.01
train_frac <- 0.8
cal_frac <- 0.3 # Of the training data, how much for calibration of marginal effects?
source("modeling.r")
}
# =========================================================================
# =========================================================================
#                         Boosting GLM
# =========================================================================
# =========================================================================
# INIT
setwd("//e75a0679/sakta/UTV/SU/Program/Analys/Boosting GLM/Boosting-GLM")
source("load_packages.r")
load_packages(updateR = FALSE)
# Output parameters --------------------------------------------------
date <- "20220403"
for (data in c( "auspriv","freMTPL","beMTPL", "norauto")){ #  REAL
rm(list = setdiff(ls(),c("data","date")))
suffix <- data
new_models <- TRUE
new_univar_effects <- TRUE
new_twoway_effects <- TRUE
save <- TRUE
scoring_boosting_factors <- TRUE
plot_folder <- paste("Plottar/",suffix,"_temp_",date, sep="")
dir.create(plot_folder)
# Modeling hyperparameters ---------------------------------------------------------
n_trees_mu <- 1000
tree_depth_mu <- 2
learning_rate_mu <- 0.01
train_frac <- 0.8
cal_frac <- 0.3 # Of the training data, how much for calibration of marginal effects?
#source("modeling.r")
source("vis_and_eval.r")
}
data
